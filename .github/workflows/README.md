I documented the Python and Scala code about the Spark Defintive Guide at this github repo. 
It's purely for personal learning purpose.
The authors have all the copyrights.

----------------------------------------------
Who this book is for

We designed this book mainly for data scientist and data engineers looking to use Apache Spark. The two roles have slighly different needs, but in reality, most application development covers a bit of both, so we think the material will be useful in both cases. Specifically, in our minds, the daa scientist workload focuses more on interactively querying data to answer questions and build statistical models, while the data engineer job focuses on writing maintainable, repeatable production applications - either to use the data scientist's models in pratice, or just to prepare data for futher analysis (e.g.,building a data ingest pipeline). However, we often see with Spark that these roles blur. For instance, data scientist are able to package production applications without too much hassle and data engineers use interactive analysis to understand and inspect their data to build and maintain pipelines.

While we tried to provide everything data scientists and engineers need to get started, there are some things we didn't have space to focus in this book. First, this book does not include in-depth introductions to some of the analytics techniques you can in Apache Spark, such as machine learning. Instead, we show you how to invoke these techniques using libraries in Spark, assuming you already have a basic background in machine learning. Many full, standalone books exist to cover these techniques in formal detail, so we recommend starting with those if you want to learn about these areas. Second, this book focuses more on application develpment than on operations and administration (e.g., how to manage an Apache Spark cluster with dozens of users). Nonetheless, we have tried to included comprehensive material on monitoring, debugging, and configuration, in Parts V and VI of the book to help engineers get their application running efficiently and tackle day-to-day maintenance. Finally, this book places less emphasis on the older, lower-level APIs in Spark - specifically RDDs and Dstreams- to introduce most of the concepts using the newer, higher-level structures APIs. Thus, thiis book may not be the best fit if you need to maintain  an old RDDs and Dstreams, but should be a great introduction to writing new applications.
